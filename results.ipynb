{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import urllib.parse\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn', Mutes warnings when copying a slice from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TSV file, ignoring lines that start with '#'\n",
    "articles = pd.read_csv('data/wikispeedia_paths-and-graph/articles.tsv', sep='\\t', comment='#')\n",
    "categories = pd.read_csv('data/wikispeedia_paths-and-graph/categories.tsv', sep='\\t', comment='#')\n",
    "links = pd.read_csv('data/wikispeedia_paths-and-graph/links.tsv', sep='\\t', comment='#')\n",
    "paths_finished = pd.read_csv('data/wikispeedia_paths-and-graph/paths_finished.tsv', sep='\\t', comment='#')\n",
    "paths_unfinished = pd.read_csv('data/wikispeedia_paths-and-graph/paths_unfinished.tsv', sep='\\t', comment='#')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_distance_matrix(filepath):\n",
    "    # Read the file and create matrix\n",
    "    matrix = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            # Skip comment lines and empty lines\n",
    "            if line.startswith('#') or not line.strip():\n",
    "                continue\n",
    "            # Convert each character to integer, ignoring underscores\n",
    "            row = [int(char.replace('_', '-1')) for char in line.strip()]\n",
    "            matrix.append(row)\n",
    "    \n",
    "    return np.array(matrix)\n",
    "\n",
    "filepath = \"data/wikispeedia_paths-and-graph/shortest-path-distance-matrix.txt\"\n",
    "distance_matrix = load_distance_matrix(filepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.columns = ['article']\n",
    "articles['article'] = articles['article'].apply(lambda x: urllib.parse.unquote(x).replace('_', ' '))\n",
    "categories.columns =['article', 'category']\n",
    "categories['category'] = categories['category'].str.replace(r'^subject\\.', '', regex=True)\n",
    "links.columns = ['linkSource', 'linkTarget']\n",
    "paths_finished.columns = ['hashedIpAddress', 'timestamp', 'durationInSec', 'path', 'rating']\n",
    "paths_unfinished.columns = ['hashedIpAddress', 'timestamp', 'durationInSec', 'path', 'target', 'type']\n",
    "paths_unfinished['target'] = paths_unfinished['target'].apply(lambda x: urllib.parse.unquote(x).replace('_', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_finished['status'] = 'finished'\n",
    "paths_unfinished['status'] = 'unfinished'\n",
    "\n",
    "# Concatenate the dataframes with the added 'status' column\n",
    "concatenated_df = pd.concat(\n",
    "    [\n",
    "        paths_finished[['hashedIpAddress', 'timestamp', 'durationInSec', 'path', 'rating', 'status']],\n",
    "        paths_unfinished[['hashedIpAddress', 'timestamp', 'durationInSec', 'path', 'target', 'type', 'status']]\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of concatenated_df to avoid modifying the original DataFrame\n",
    "clean_merge = concatenated_df.copy()\n",
    "\n",
    "# Modify 'durationInSec' in the new DataFrame based on the condition\n",
    "for index, row in clean_merge.iterrows():\n",
    "    if row['type'] == 'timeout':  # Check if the last element in 'path' is 'timeout'\n",
    "        clean_merge.loc[index, 'durationInSec'] -= 1800  # Subtract 1800 from 'durationInSec'\n",
    "\n",
    "def find_backtracks(path):\n",
    "    backtracks = []\n",
    "    words = path.split(';')\n",
    "    stack = []\n",
    "\n",
    "    for word in words:\n",
    "        if word == \"<\":\n",
    "            if stack:\n",
    "                backtracks.append(stack.pop())\n",
    "        else:\n",
    "            url_decoded = urllib.parse.unquote(word).replace('_', ' ')\n",
    "            stack.append(url_decoded)\n",
    "\n",
    "    return backtracks\n",
    "\n",
    "paths_finished['backtracks'] = paths_finished['path'].apply(find_backtracks)\n",
    "paths_unfinished['backtracks'] = paths_unfinished['path'].apply(find_backtracks)\n",
    "clean_merge['backtracks'] = clean_merge['path'].apply(find_backtracks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect ratings for paths with and without backtracking\n",
    "filtered_ratings_back = []\n",
    "filtered_ratings = []\n",
    "\n",
    "for index, row in paths_finished.iterrows():\n",
    "    if '<' in row['path'].split(';') and pd.notna(row['rating']):\n",
    "        filtered_ratings_back.append(row['rating'])  # Paths with backtracking\n",
    "    elif '<' not in row['path'].split(';') and pd.notna(row['rating']):\n",
    "        filtered_ratings.append(row['rating'])  # Paths without backtracking\n",
    "\n",
    "# Calculate the average for each group\n",
    "average_back = sum(filtered_ratings_back) / len(filtered_ratings_back) if filtered_ratings_back else 0\n",
    "average_no_back = sum(filtered_ratings) / len(filtered_ratings) if filtered_ratings else 0\n",
    "\n",
    "print(\"Average rating (with backtracking):\", average_back)\n",
    "print(\"Average rating (without backtracking):\", average_no_back)\n",
    "\n",
    "# Plot the histograms\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot histogram for paths with backtracking\n",
    "plt.hist(filtered_ratings_back, bins=5, color='orange', edgecolor='black', alpha=0.7, label='With Backtracking')\n",
    "\n",
    "# Plot histogram for paths without backtracking\n",
    "plt.hist(filtered_ratings, bins=5, color='skyblue', edgecolor='black', alpha=0.7, label='Without Backtracking')\n",
    "\n",
    "# Add vertical lines for the averages\n",
    "plt.axvline(x=average_back, color='red', linestyle='-', linewidth=2, label=f'Average (with backtracking) = {average_back:.2f}')\n",
    "plt.axvline(x=average_no_back, color='blue', linestyle='-', linewidth=2, label=f'Average (without backtracking) = {average_no_back:.2f}')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Ratings for Paths With and Without Backtracking')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Across the study, we have {len(paths_finished)} paths, while {len(paths_unfinished)} remain unfinished, for a total of {len(clean_merge)} paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average number of backtracks for finished vs unfinished paths\n",
    "avg_backtracks_finished = clean_merge[clean_merge['status'] == 'finished']['backtracks'].apply(len).mean()\n",
    "avg_backtracks_unfinished = clean_merge[clean_merge['status'] != 'finished']['backtracks'].apply(len).mean()\n",
    "\n",
    "# Output the results\n",
    "print(\"\\nAverage number of backtracks per game:\")\n",
    "print(f\"Finished paths: {avg_backtracks_finished:.2f}\")\n",
    "print(f\"Unfinished paths: {avg_backtracks_unfinished:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get finished games with and without backtracks\n",
    "\n",
    "finished_with_bt = paths_finished[paths_finished['backtracks'].apply(len) > 0]\n",
    "finished_no_bt = paths_finished[paths_finished['backtracks'].apply(len) == 0]\n",
    "\n",
    "# Calculate average duration for each group\n",
    "avg_duration_with_bt = finished_with_bt['durationInSec'].median()\n",
    "avg_duration_no_bt = finished_no_bt['durationInSec'].median()\n",
    "\n",
    "# Group finished games by number of backtracks and calculate median duration\n",
    "backtrack_counts = paths_finished.groupby(paths_finished['backtracks'].apply(len)).size()\n",
    "backtrack_durations = paths_finished.groupby(paths_finished['backtracks'].apply(len))['durationInSec'].median()\n",
    "\n",
    "# Filter to keep only counts with at least 10 samples\n",
    "filtered_counts = backtrack_counts[backtrack_counts >= 10]\n",
    "filtered_durations = backtrack_durations[filtered_counts.index]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(filtered_durations.index, filtered_durations.values, marker='o')\n",
    "plt.axvline(x=avg_backtracks_finished, color='r', linestyle='--', label='Average backtracks')\n",
    "plt.axhline(y=avg_duration_with_bt, color='g', linestyle='--', label='Average duration')\n",
    "plt.xlabel('Number of Backtracks')\n",
    "plt.ylabel('Median Duration (seconds)')\n",
    "plt.title('Median Game Duration vs Number of Backtracks (10+ samples)')\n",
    "plt.grid(True)\n",
    "\n",
    "# Add a trend line\n",
    "z = np.polyfit(filtered_durations.index, filtered_durations.values, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(filtered_durations.index, p(filtered_durations.index), \"r--\", alpha=0.8, \n",
    "         label=f'Trend line (slope: {z[0]:.1f})')\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = r2_score(filtered_durations.values, p(filtered_durations.index))\n",
    "\n",
    "plt.legend()\n",
    "plt.text(0.05, 0.95, f'R-squared: {r_squared:.4f}', transform=plt.gca().transAxes, \n",
    "         verticalalignment='top')\n",
    "plt.show()\n",
    "\n",
    "print(\"Counts of games with each number of backtracks:\")\n",
    "print(filtered_counts)\n",
    "print(f\"\\nR-squared value: {r_squared:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group finished games by number of backtracks and calculate median duration\n",
    "backtrack_counts = paths_finished.groupby(paths_finished['backtracks'].apply(len)).size()\n",
    "backtrack_ratings = paths_finished.groupby(paths_finished['backtracks'].apply(len))['rating'].mean()\n",
    "\n",
    "# Filter to keep only counts with at least 10 samples\n",
    "filtered_counts = backtrack_counts[backtrack_counts >= 10]\n",
    "filtered_ratings = backtrack_ratings[filtered_counts.index]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(filtered_ratings.index, filtered_ratings.values, marker='o')\n",
    "plt.xlabel('Number of Backtracks')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.axvline(x=avg_backtracks_finished, color='r', linestyle='--', label='Average backtracks')\n",
    "plt.axhline(y=paths_finished['rating'].mean(), color='g', linestyle='--', label='Average rating')\n",
    "plt.title('Average Rating vs Number of Backtracks (10+ samples)')\n",
    "plt.grid(True)\n",
    "\n",
    "# Add a trend line\n",
    "z = np.polyfit(filtered_ratings.index, filtered_ratings.values, 1)\n",
    "p = np.poly1d(z)\n",
    "\n",
    "plt.plot(filtered_ratings.index, p(filtered_ratings.index), \"r--\", alpha=0.8, \n",
    "         label=f'Trend line (slope: {z[0]:.1f})')\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = r2_score(filtered_ratings.values, p(filtered_ratings.index))\n",
    "\n",
    "plt.legend()\n",
    "plt.text(0.05, 0.95, f'R-squared: {r_squared:.4f}', transform=plt.gca().transAxes, \n",
    "         verticalalignment='top')\n",
    "plt.show()\n",
    "\n",
    "print(\"Counts of games with each number of backtracks:\")\n",
    "print(filtered_counts)\n",
    "print(f\"\\nR-squared value: {r_squared:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a DataFrame with counts of backtracks and traversed\n",
    "backtracks = pd.Series(clean_merge['backtracks'].explode().dropna().tolist())\n",
    "def path_to_list(path):\n",
    "    return [urllib.parse.unquote(word).replace('_', ' ') for word in path.replace('<;', '').split(';')]\n",
    "\n",
    "clean_merge[\"traversed\"] = clean_merge[\"path\"].apply(path_to_list)\n",
    "traversed = pd.Series(clean_merge['traversed'].explode().dropna().tolist())\n",
    "\n",
    "word_counts = pd.DataFrame({\n",
    "    'backtracks': backtracks.value_counts(),\n",
    "    'traversed': traversed.value_counts()\n",
    "})\n",
    "\n",
    "# Calculate the ratio of backtracks to traversed\n",
    "word_counts['backtrack_ratio'] = word_counts['backtracks'] / word_counts['traversed']\n",
    "\n",
    "# Fill NaN values with 0 (for words that were traversed but never backtracked)\n",
    "word_counts['backtrack_ratio'] = word_counts['backtrack_ratio'].fillna(0)\n",
    "\n",
    "# Sort by the ratio in descending order\n",
    "word_counts_sorted = word_counts.sort_values('backtrack_ratio', ascending=False)\n",
    "\n",
    "# Display the top 20 words with highest backtrack ratios for words with at least 100 occurrences\n",
    "print(word_counts_sorted[word_counts_sorted['traversed'] >= 0].head(20))\n",
    "\n",
    "# Optional: Plot the distribution of backtrack ratios\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(word_counts['backtrack_ratio'], bins=50, edgecolor='black')\n",
    "plt.title('Distribution of Backtrack Ratios')\n",
    "plt.xlabel('Backtrack Ratio')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_unfinished.groupby('type').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of games in each 'type' group\n",
    "game_counts_by_type = paths_unfinished.groupby('type').size()\n",
    "\n",
    "# Display the result\n",
    "print(game_counts_by_type)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all backtrack targets\n",
    "backtrack_targets = clean_merge[clean_merge['backtracks'].apply(len) > 0][['backtracks','target']]\n",
    "print(backtrack_targets.head())\n",
    "# Create a list to store individual backtrack entries\n",
    "backtrack_entries = []\n",
    "\n",
    "# Iterate through each row that has backtracks\n",
    "for _, row in backtrack_targets.iterrows():\n",
    "    # For each backtrack in the row\n",
    "    for backtrack in row['backtracks']:\n",
    "        # Add an entry with the backtrack and target\n",
    "        backtrack_entries.append({\n",
    "            'backtrack': backtrack,\n",
    "            'target': row['target']\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "backtrack_df = pd.DataFrame(backtrack_entries)\n",
    "\n",
    "print(\"\\nArticle of backtrack vs target\")\n",
    "print(backtrack_df.value_counts().head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing targets more efficiently and store them in a list\n",
    "articles_set = set(articles['article'])\n",
    "missing_targets = [target for target in paths_unfinished['target'] if target not in articles_set]\n",
    "\n",
    "# Print missing targets if any\n",
    "if missing_targets:\n",
    "    print(\"Missing targets:\")\n",
    "    for target in missing_targets:\n",
    "        print(target)\n",
    "else:\n",
    "    print(\"No missing targets found.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_path_similarities = pd.read_csv('data/similarities.csv')\n",
    "all_path_similarities['last_article'] = all_path_similarities['path'].apply(lambda x: eval(x)[0][-1])\n",
    "all_path_similarities_unfinished = all_path_similarities[all_path_similarities['last_article'] != all_path_similarities['target']]\n",
    "all_path_similarities_finished = all_path_similarities[all_path_similarities['last_article'] == all_path_similarities['target']]\n",
    "all_path_similarities_finished.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only the last 10 elements of each path\n",
    "def shorten_list(x,n=10):\n",
    "    x = eval(x.replace(\" \", \"\"))  # Convert string representation of list to actual list\n",
    "    x=x[0] #unpack the list\n",
    "    if len(x) > n:\n",
    "        return x[-n:]\n",
    "    return x\n",
    "\n",
    "def handle_unfinished(entry,n=9):\n",
    "    similarities = eval(entry['OpenAI'].replace(\" \", \"\"))  # Convert string representation of list to actual list\n",
    "    path = eval(entry['path'])\n",
    "    similarities = similarities[0] #unpack the list\n",
    "    path = path[0]\n",
    "\n",
    "    max_value = max(similarities)\n",
    "    if max_value < 0.3:\n",
    "        return [],[]\n",
    "    max_index = similarities.index(max_value)\n",
    "    similarities = similarities[:max_index + 1]  # Keep only values up to and including max\n",
    "    path = path[:max_index + 1]\n",
    "    if len(similarities) > n:\n",
    "        return similarities[-n:],path[-n:]\n",
    "    return similarities,path\n",
    "\n",
    "all_path_similarities_unfinished_copy = all_path_similarities_unfinished.copy()\n",
    "all_path_similarities_unfinished_copy['OpenAI'],all_path_similarities_unfinished_copy['path'] = zip(*all_path_similarities_unfinished_copy.apply(handle_unfinished, axis=1))\n",
    "\n",
    "similarities_unfinished = all_path_similarities_unfinished_copy[all_path_similarities_unfinished_copy['OpenAI'].apply(lambda x: len(x) > 0)][['OpenAI','path','target']]\n",
    "print(similarities_unfinished.head())\n",
    "\n",
    "\n",
    "all_path_similarities_finished_copy = all_path_similarities_finished.copy()\n",
    "all_path_similarities_finished_copy['OpenAI'] = all_path_similarities_finished_copy['OpenAI'].apply(shorten_list)\n",
    "similarities_finished = all_path_similarities_finished_copy['OpenAI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_links(entry):\n",
    "    # Return a Series instead of a tuple\n",
    "    return pd.Series({\n",
    "        'biggest_similarity_article': entry['path'][-1],\n",
    "        'target': entry['target']\n",
    "    })\n",
    "\n",
    "unfinished_missing_links = similarities_unfinished.apply(get_missing_links, axis=1)\n",
    "print(\"Most common pairs of articles that may be missing links:\")\n",
    "print(unfinished_missing_links.value_counts().head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = all_path_similarities\n",
    "# Create a figure with subplots for the first 10 paths\n",
    "fig, axes = plt.subplots(5, 2, figsize=(15, 25))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Colors for each model\n",
    "colors = {\n",
    "    'OpenAI': 'green'\n",
    "}\n",
    "\n",
    "# Plot first 10 paths\n",
    "for i in range(10):\n",
    "    path = final_results.apply(lambda x: eval(x['path']),axis=1).iloc[i][0]\n",
    "    target = final_results['target'].iloc[i]\n",
    "    \n",
    "    # Plot each model's similarities\n",
    "    for model in ['OpenAI']:\n",
    "        similarities = final_results.apply(lambda x: eval(x[model]),axis=1).iloc[i][0]\n",
    "        axes[i].plot(range(len(similarities)), similarities, \n",
    "                    marker='o', label=model, color=colors[model])\n",
    "    \n",
    "    # Customize the plot\n",
    "    axes[i].set_title(f'Path {i+1}\\nTarget: {target}')\n",
    "    axes[i].set_xlabel('Step in Path')\n",
    "    axes[i].set_ylabel('Cosine Similarity')\n",
    "    axes[i].set_ylim(0, 1)\n",
    "    axes[i].grid(True)\n",
    "    axes[i].legend()\n",
    "    \n",
    "    # Add path words as x-tick labels\n",
    "    axes[i].set_xticks(range(len(path)))\n",
    "    axes[i].set_xticklabels(path, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axis\n",
    "plt.figure(figsize=(15, 8))\n",
    "n=10\n",
    "\n",
    "# Plot first 100 similarity evolutions\n",
    "for i in range(1000):\n",
    "    similarities = all_path_similarities_finished_copy['OpenAI'].iloc[i]\n",
    "    reversed_similarities = similarities[::-1]\n",
    "    indexes = range(n,n-len(reversed_similarities),-1)\n",
    "    plt.plot(indexes, reversed_similarities)\n",
    "\n",
    "\n",
    "plt.xlabel('distance to end of game')\n",
    "plt.ylabel('Similarity Score') \n",
    "plt.title('Evolution of Similarities for First 100 Paths')\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_to_series_finished ={}\n",
    "for i in tqdm(range(len(similarities_finished))):\n",
    "    similarities = similarities_finished.iloc[i]\n",
    "    reversed_similarities = similarities[::-1]\n",
    "    for j in range(len(reversed_similarities)):\n",
    "        distance_to_series_finished[j] = distance_to_series_finished.get(j,[]) + [reversed_similarities[j]]\n",
    "\n",
    "print(len(distance_to_series_finished))\n",
    "\n",
    "distance_to_series_unfinished ={}\n",
    "similarities_unfinished = similarities_unfinished['OpenAI']\n",
    "for i in tqdm(range(len(similarities_unfinished))):\n",
    "    similarities = similarities_unfinished.iloc[i]\n",
    "    reversed_similarities = similarities[::-1]\n",
    "    for j in range(len(reversed_similarities)):\n",
    "        distance_to_series_unfinished[j] = distance_to_series_unfinished.get(j,[]) + [reversed_similarities[j]]\n",
    "\n",
    "print(distance_to_series_unfinished)\n",
    "\n",
    "\n",
    "l_finished=[]\n",
    "for i in range(len(distance_to_series_finished)):\n",
    "    l_finished.append(distance_to_series_finished[i])\n",
    "\n",
    "l_unfinished=[]\n",
    "for i in range(len(distance_to_series_unfinished)):\n",
    "    l_unfinished.append(distance_to_series_unfinished[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Calculate means and standard deviations\n",
    "means_finished = [np.mean(sublist) for sublist in l_finished]\n",
    "medians_finished = [np.median(sublist) for sublist in l_finished]\n",
    "stds_finished = [np.std(sublist) for sublist in l_finished]\n",
    "\n",
    "means_unfinished = [np.mean(sublist) for sublist in l_unfinished]\n",
    "medians_unfinished = [np.median(sublist) for sublist in l_unfinished]\n",
    "stds_unfinished = [np.std(sublist) for sublist in l_unfinished]\n",
    "\n",
    "\n",
    "# Create x-axis values (distances)\n",
    "x_finished = range(len(means_finished)-1, -1, -1)  # Reversed range\n",
    "x_unfinished = range(len(means_unfinished)-1, -1, -1)  # Reversed range\n",
    "\n",
    "# Create figure and axis\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot means with standard deviation areas\n",
    "plt.plot(x_finished, means_finished, color='#1f77b4', label='Mean (Finished)', linewidth=2)\n",
    "plt.plot(x_finished, medians_finished, color='#ff7f0e', label='Median (Finished)', linewidth=2)\n",
    "plt.fill_between(x_finished, \n",
    "                 [m - s for m,s in zip(means_finished, stds_finished)],\n",
    "                 [m + s for m,s in zip(means_finished, stds_finished)],\n",
    "                 color='#1f77b4', alpha=0.2)\n",
    "\n",
    "plt.plot(x_unfinished, means_unfinished, color='#2ca02c', label='Mean (Unfinished)', linewidth=2)\n",
    "plt.plot(x_unfinished, medians_unfinished, color='#d62728', label='Median (Unfinished)', linewidth=2)\n",
    "plt.fill_between(x_unfinished, \n",
    "                 [m - s for m,s in zip(means_unfinished, stds_unfinished)],\n",
    "                 [m + s for m,s in zip(means_unfinished, stds_unfinished)],\n",
    "                 color='#2ca02c', alpha=0.2)\n",
    "\n",
    "plt.xlabel('Distance from End', fontsize=12)\n",
    "plt.ylabel('Average Similarity Score', fontsize=12)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.title('Mean and Median Similarity Score vs Distance from End (with Standard Deviation)', fontsize=14)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(fontsize=10)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_links_df =pd.read_csv('data/chosen_links_rank.csv')\n",
    "\n",
    "\n",
    "rank_data = chosen_links_df['RankChosen'].dropna()\n",
    "\n",
    "# Count the frequency of each rank\n",
    "rank_counts = rank_data.value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(rank_counts.index, rank_counts.values, color='skyblue')\n",
    "plt.xlabel('Position of Chosen Link in Article')\n",
    "plt.xlim(1, 30)  # Limit to first 20 positions\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Chosen Links Based on Position in Article')\n",
    "\n",
    "# Set x-ticks to show every position within the limited range\n",
    "plt.xticks(range(1, 31))  # Adjust as necessary\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Replace -1 values with np.nan to ignore them during normalization\n",
    "distance_matrix_normalized = np.where(distance_matrix == -1, np.nan, distance_matrix)\n",
    "\n",
    "# Step 2: Perform normalization on the matrix, excluding NaN values\n",
    "min_value = np.nanmin(distance_matrix_normalized)\n",
    "max_value = np.nanmax(distance_matrix_normalized)\n",
    "distance_matrix_normalized = (distance_matrix_normalized - min_value) / (max_value - min_value)\n",
    "\n",
    "# Step 3: Replace NaNs (original -1 values) with 1\n",
    "distance_matrix_normalized = np.where(np.isnan(distance_matrix_normalized), 1, distance_matrix_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = np.nanmin(distance_matrix_normalized)\n",
    "max_value = np.nanmax(distance_matrix_normalized)\n",
    "distance_matrix_normalized = (distance_matrix_normalized - min_value) / (max_value - min_value)\n",
    "\n",
    "# Step 3: Replace NaNs (original -1 values) with 1\n",
    "distance_matrix_normalized = np.where(np.isnan(distance_matrix_normalized), 1, distance_matrix_normalized)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(distance_matrix_normalized, cmap=\"YlGnBu\", annot=False, cbar=True)\n",
    "plt.title('Heatmap of Normalized Distance Matrix')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Rows')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
